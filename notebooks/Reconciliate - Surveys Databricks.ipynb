{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a204e8c2-416b-439f-9ec1-35ff787599fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Notebook Documentation\n",
    "\n",
    "## Goal\n",
    "The purpose of this notebook is to facilitate the upload and processing of background data from various survey providers, including Cint Access, Syno Distribution, Lucid, and PureSpectrum. Additionally, the notebook will handle the import of .sav files containing exported raw data, including non-completes, for analysis.\n",
    "\n",
    "## Process Overview\n",
    "1. **Data Upload**: Users will upload background data from the specified providers. This data typically includes respondent IDs, survey completion status, and other metadata.\n",
    "2. **Data Processing**: The notebook will process the uploaded data to prepare it for analysis. This may involve data cleaning, transformation, and merging of datasets from different sources.\n",
    "3. **Analysis of .sav Files**: The notebook will import and analyze .sav files, which are data files from SPSS (Statistical Package for the Social Sciences). These files contain survey responses, including those from respondents who did not complete the survey.\n",
    "4. **Reconciliation**: The final step is to generate an Excel file containing IDs that will be used to reconcile responses with the data collected by the Syno Survey tool.\n",
    "\n",
    "## Expected Outputs\n",
    "- An Excel file with reconciled IDs for matching survey responses with the collected data.\n",
    "\n",
    "## User Instructions\n",
    "- Users should upload all background data files from the specified providers to the notebook.\n",
    "- Users should also provide all .sav files from the exported raw data for analysis.\n",
    "- The notebook will process the data and generate the required Excel file for reconciliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3610fa39-0337-4e2d-9cb5-800ff26f6d3d",
     "showTitle": true,
     "title": "Import the libraries"
    },
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 21,
    "lastExecutedAt": 1708746645431,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pyreadstat\nfrom datetime import datetime\nimport pandas as pd \nimport numpy as np \nimport os"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pyreadstat\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Uncomment if using an .env file to store secrets\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dce04b47-aae1-456b-ae22-b5f5e7bf4fa2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Read the data in SPSS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd5a4f9-ae27-494e-9a3b-e2fbb088f7fe",
     "showTitle": false,
     "title": ""
    },
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1708746773723,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def read_data(directory = \"data\"):\n    \"\"\"\n    Read the data from the specified folder\n    \n    Params: \n        directory\n            - Default (data), if not specified. Can be overwritten by another location\n    \"\"\"\n    print(f\"Reading from directory: {directory}\")\n    data = pd.DataFrame()\n\n    filenames = os.listdir(directory)\n    for filename in filenames:\n        print(f\"Reading file: {filename}\")\n        sav, meta = pyreadstat.read_sav(f\"{directory}/{filename}\", apply_value_formats=True)\n        data = pd.concat([data, sav], axis = 0)\n        \n    return data",
    "outputsMetadata": {
     "0": {
      "height": 177,
      "type": "stream"
     },
     "1": {
      "height": 97,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "def read_data(directory = os.environ[\"DATA_DIR\"]):\n",
    "    \"\"\"\n",
    "    Read the data from the specified folder\n",
    "    \n",
    "    Params: \n",
    "        directory\n",
    "            - Default (data), if not specified. Can be overwritten by another location\n",
    "    \"\"\"\n",
    "    print(f\"Reading from directory: {directory}\")\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    filenames = os.listdir(directory)\n",
    "    for filename in filenames:\n",
    "        print(f\"Reading file: {filename}\")\n",
    "        sav, meta = pyreadstat.read_sav(f\"{directory}/{filename}\", apply_value_formats=True)\n",
    "        data = pd.concat([data, sav], axis = 0)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5381a6-88f5-4b7f-ae78-c3a3e0855937",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 18004,
    "lastExecutedAt": 1708746794667,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "data = read_data()",
    "outputsMetadata": {
     "0": {
      "height": 197,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from directory: data/\n",
      "Reading file: Raw Data 156910 2024-02-24.sav\n",
      "Reading file: Raw Data 174031 2024-02-24.sav\n",
      "Reading file: Raw Data 222861 2024-02-24.sav\n",
      "Reading file: Raw Data 665730 2024-02-24.sav\n",
      "Reading file: Raw Data 818418 2024-02-24.sav\n"
     ]
    }
   ],
   "source": [
    "data = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a68589bd-d1f2-46e8-9702-e1b04fafc872",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Read the distribution files and merge them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26dffdf0-d449-442f-bba0-b85ae591e4e0",
     "showTitle": false,
     "title": ""
    },
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1708746802725,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def read_distribution(directory = \"distribution\"):\n    \"\"\"\n    Reads a csv file from a specified directory\n    \n    Params: \n        directory\n            - By default the directory `data` is read. It can be overwritten by another location\n    \"\"\"\n    print(f\"Reading from directory: {directory}\")\n\n    # Concat all distribution files\n    distribution_bg = pd.DataFrame()\n\n    for distribution_file in os.listdir(directory):\n        print(f\"Reading file: {distribution_file}\")\n        # Read the current filename \n        df = pd.read_csv(\"distribution/\" + distribution_file)\n        # Filter out data of interesst\n        df.drop(\"Unnamed: 8\", axis = \"columns\", inplace=True)\n        df.rename(columns={\"GUID\" : \"guid\"}, inplace=True)\n        # Concat dataframes by rows\n        distribution_bg = pd.concat([distribution_bg, df], axis = \"index\")\n\n    return distribution_bg",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     },
     "1": {
      "height": 323,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "def read_distribution(directory = os.environ[\"DISTRIBUTION_DIR\"]):\n",
    "    \"\"\"\n",
    "    Reads a csv file from a specified directory\n",
    "    \n",
    "    Params: \n",
    "        directory\n",
    "            - By default the directory `data` is read. It can be overwritten by another location\n",
    "    \"\"\"\n",
    "    print(f\"Reading from directory: {directory}\")\n",
    "\n",
    "    # Concat all distribution files\n",
    "    distribution_bg = pd.DataFrame()\n",
    "\n",
    "    for distribution_file in os.listdir(directory):\n",
    "        print(f\"Reading file: {distribution_file}\")\n",
    "        # Read the current filename \n",
    "        df = pd.read_csv(directory + distribution_file)\n",
    "        # Filter out columns of interesst\n",
    "        df.drop(\"Unnamed: 8\", axis = \"columns\", inplace=True)\n",
    "        df.rename(columns={\"GUID\" : \"guid\"}, inplace=True)\n",
    "        # Concat dataframes by rows\n",
    "        distribution_bg = pd.concat([distribution_bg, df], axis = \"index\")\n",
    "\n",
    "    return distribution_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6542162-6dd2-4c89-bd38-df0fc0252875",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 27,
    "lastExecutedAt": 1708746805419,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "distribution = read_distribution()",
    "outputsMetadata": {
     "0": {
      "height": 57,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from directory: distribution/\n",
      "Reading file: 4300 Encuesta Satisfacción LATAM sept 2023_2024-02-24.csv\n",
      "Reading file: P4300 Encuesta Satisfacción Latam - January 2024 - Reinvites_2024-02-24.csv\n",
      "Reading file: P4300 Encuesta Satisfacción Latam - January 2024_2024-02-24.csv\n"
     ]
    }
   ],
   "source": [
    "distribution = read_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21f64fe5-32a7-4a66-b03e-a80f5a0bcea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Read the purespectrum files and merge them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2656f74-ecf5-4a4e-9245-340f15960469",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 14,
    "lastExecutedAt": 1708746811919,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def read_purespectrum(directory = \"purespectrum\"):\n    print(f\"Reading from directory: {directory}\")\n    \n    # Concat all closed files\n    purespectrum_bg = pd.DataFrame()\n\n    for purespectrum_file in os.listdir(directory):\n        print(f\"Reading file: {purespectrum_file}\")\n        # Read the current filename \n        df = pd.read_csv(\"purespectrum/\" + purespectrum_file)\n        # Filter out data of interesst\n        df = df[[\"Survey ID\", \"Project Name\", \"PSID\", \"Transaction ID\", \"Survey Country\", \"Survey Language\", \"Respondent Status Description\", \"IP\", \"UserAgent\"]].reset_index(drop=True)\n        df.rename(columns={\"Transaction ID\" : \"guid\"}, inplace=True)\n        # Concat dataframes by rows\n        purespectrum_bg = pd.concat([purespectrum_bg, df], axis = \"index\")\n\n    return purespectrum_bg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_purespectrum(directory = os.environ[\"PURESPECTRUM_DIR\"]):\n",
    "    print(f\"Reading from directory: {directory}\")\n",
    "    \n",
    "    # Concat all closed files\n",
    "    purespectrum_bg = pd.DataFrame()\n",
    "\n",
    "    for purespectrum_file in os.listdir(directory):\n",
    "        print(f\"Reading file: {purespectrum_file}\")\n",
    "        # Read the current filename \n",
    "        df = pd.read_csv(directory + purespectrum_file)\n",
    "        # Filter out columns of interest\n",
    "        df = df[[\"Survey ID\", \"Project Name\", \"PSID\", \"Transaction ID\", \"Survey Country\", \"Survey Language\", \"Respondent Status Description\", \"IP\", \"UserAgent\"]].reset_index(drop=True)\n",
    "        df.rename(columns={\"Transaction ID\" : \"guid\"}, inplace=True)\n",
    "        # Concat dataframes by rows\n",
    "        purespectrum_bg = pd.concat([purespectrum_bg, df], axis = \"index\")\n",
    "\n",
    "    return purespectrum_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51947ce6-d64b-4bcc-9cf5-621b8e1eaf98",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 106,
    "lastExecutedAt": 1708746814003,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "purespectrum = read_purespectrum()",
    "outputsMetadata": {
     "0": {
      "height": 57,
      "type": "stream"
     },
     "1": {
      "height": 97,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from directory: purespectrum/\n",
      "Reading file: survey_All_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f85056f19118>:10: DtypeWarning: Columns (67,68,69,70,71,72) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(directory + purespectrum_file)\n"
     ]
    }
   ],
   "source": [
    "purespectrum = read_purespectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1774d54e-0848-4520-9817-3103f0c341ed",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1708746838279,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def pureRemoves(pure, data):\n    # Completes that we must have as complete in PureSpectrum\n    pure_match = pure[pure[\"guid\"].isin( data[ (data[\"source\"] == \"Pure Spectrum\") & (data[\"status\"] == \"complete\") & (data[\"mode\"] == \"live\") ][\"guid\"] )]\n    # Respondents to remove in PureSpectrum\n    pure_removes = pure[\n        (~pure[\"guid\"].isin( pure_match[\"guid\"])) &  # Filter non matched Survey completes\n        (pure[\"Respondent Status Description\"] == \"Complete\") # Filter only completes in Pure that did not matched in Surveys\n    ]\n\n    return pure_removes",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pureRemoves(pure, data):\n",
    "    # Completes that we must have as complete in PureSpectrum\n",
    "    pure_match = pure[pure[\"guid\"].isin( data[ (data[\"source\"] == \"Pure Spectrum\") & (data[\"status\"] == \"complete\") & (data[\"mode\"] == \"live\") ][\"guid\"] )]\n",
    "    # Respondents to remove in PureSpectrum\n",
    "    pure_removes = pure[\n",
    "        (~pure[\"guid\"].isin( pure_match[\"guid\"])) &  # Filter non matched Survey completes\n",
    "        (pure[\"Respondent Status Description\"] == \"Complete\") # Filter only completes in Pure that did not matched in Surveys\n",
    "    ]\n",
    "\n",
    "    return pure_removes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e088e6-bdab-42ff-8322-c7121fb893d1",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1708746841292,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def pureAdds(pure, data):\n    # Completes that we must have as complete in PureSpectrum\n    pure_match = pure[pure[\"guid\"].isin( data[ (data[\"source\"] == \"Pure Spectrum\") & (data[\"status\"] == \"complete\") & (data[\"mode\"] == \"live\") ][\"guid\"] )]\n    # Respondents to add in PureSpectrum\n    pure_add = pure_match[pure_match[\"Respondent Status Description\"] != \"Complete\"]\n    # pure_add.to_excel(\"pure_add.xlsx\", index = False)\n    \n    return pure_add",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pureAdds(pure, data):\n",
    "    # Completes that we must have as complete in PureSpectrum\n",
    "    pure_match = pure[pure[\"guid\"].isin( data[ (data[\"source\"] == \"Pure Spectrum\") & (data[\"status\"] == \"complete\") & (data[\"mode\"] == \"live\") ][\"guid\"] )]\n",
    "    # Respondents to add in PureSpectrum\n",
    "    pure_add = pure_match[pure_match[\"Respondent Status Description\"] != \"Complete\"]\n",
    "    # pure_add.to_excel(\"pure_add.xlsx\", index = False)\n",
    "    \n",
    "    return pure_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a3f0f90-c51f-4d54-8ad4-97fa5ac772a0",
     "showTitle": false,
     "title": ""
    },
    "chartConfig": {
     "bar": {
      "hasRoundedCorners": true,
      "stacked": false
     },
     "type": "bar",
     "version": "v1"
    },
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1708746844668,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def distributionRemoves(distribution, data):\n    # Completes that we must have as completes in Distribution\n    distribution_match = distribution[distribution[\"guid\"].isin( data[ (data[\"source\"].isin([\"Cint\", \"Syno\"])) & (data[\"status\"] == \"complete\") & (data[\"mode\"] == \"live\") ][\"guid\"] )]\n\n    # Respondents to remove in Distribution\n    distribution_removes = distribution[\n        (~distribution[\"guid\"].isin( distribution_match[\"guid\"] )) &\n        (distribution[\"Status\"] == \"Complete\")\n    ]\n\n    return distribution_removes",
    "outputsMetadata": {
     "0": {
      "columns": {
       "guid": {
        "wrap": false
       }
      },
      "height": 45,
      "type": "dataFrame"
     }
    },
    "visualizeDataframe": false
   },
   "outputs": [],
   "source": [
    "def distributionRemoves(distribution, data):\n",
    "    # Completes that we must have as completes in Distribution\n",
    "    distribution_match = distribution[distribution[\"guid\"].isin( data[ (data[\"source\"].isin([\"Cint\", \"Syno\"])) & (data[\"status\"] == \"complete\") & (data[\"mode\"] == \"live\") ][\"guid\"] )]\n",
    "\n",
    "    # Respondents to remove in Distribution\n",
    "    distribution_removes = distribution[\n",
    "        (~distribution[\"guid\"].isin( distribution_match[\"guid\"] )) &\n",
    "        (distribution[\"Status\"] == \"Complete\")\n",
    "    ]\n",
    "\n",
    "    return distribution_removes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa06d6e6-6d50-4fd2-9e51-d8fa9e2d7b6b",
     "showTitle": false,
     "title": ""
    },
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1708746846331,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def distributionAdds(distribution, data):\n    # Completes that we must have as completes in Distribution\n    distribution_match = distribution[distribution[\"guid\"].isin( data[ (data[\"source\"].isin([\"Cint\", \"Syno\"])) & (data[\"status\"] == \"complete\") & (data[\"mode\"] == \"live\") ][\"guid\"] )]\n\n    # Respondents to add in Distribution\n    distribution_add = distribution_match[distribution_match[\"Status\"] != \"Complete\"]\n    \n    return distribution_add",
    "outputsMetadata": {
     "0": {
      "height": 45,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "def distributionAdds(distribution, data):\n",
    "    # Completes that we must have as completes in Distribution\n",
    "    distribution_match = distribution[distribution[\"guid\"].isin( data[ (data[\"source\"].isin([\"Cint\", \"Syno\"])) & (data[\"status\"] == \"complete\") & (data[\"mode\"] == \"live\") ][\"guid\"] )]\n",
    "\n",
    "    # Respondents to add in Distribution\n",
    "    distribution_add = distribution_match[distribution_match[\"Status\"] != \"Complete\"]\n",
    "    \n",
    "    return distribution_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea95ace-acd8-4ccf-b355-86fecdd98c02",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1708747348335,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def format_sheet(writer, sheet_name, data):\n    # Obtener el libro de trabajo y la hoja de trabajo para cambiar las propiedades\n    workbook  = writer.book\n    worksheet = writer.sheets[sheet_name]\n\n    # Cambiar la fuente de toda la tabla a Arial 9\n    cell_format = workbook.add_format({'font_name': 'Arial', 'font_size': 9})\n\n    # Cambiar el ancho y la fuente de todas las columnas\n    for col_num, value in enumerate(data.columns.values):\n        worksheet.set_column(col_num, col_num, 12.5, cell_format)"
   },
   "outputs": [],
   "source": [
    "def format_sheet(writer, sheet_name, data):\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    cell_format = workbook.add_format({'font_name': 'Arial', 'font_size': 9})  # This line is correct, assuming the engine is 'xlsxwriter'\n",
    "    for col_num, value in enumerate(data.columns.values):\n",
    "        worksheet.set_column(col_num, col_num, 12.5, cell_format)  # Example of setting column width and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc135c2-1481-4585-9f10-8a35c39766b7",
     "showTitle": false,
     "title": ""
    },
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 719,
    "lastExecutedAt": 1708747916126,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from datetime import datetime\nimport pandas as pd  # Ensure pandas is imported\n\n# Get the current date in the format MM_DD_YYYY\ncurrent_date = datetime.now().strftime(\"%m_%d_%Y\")\n\n# Use the current date in the filename\nfilename = f\"Reconciliation - {current_date}.xlsx\"\n\nwith pd.ExcelWriter(filename, engine='xlsxwriter') as writer:  # Specify the engine\n    print(\"Exporting the data to an Excel file\")\n    \n    # PureSpectrum reconciliations\n    print(\"Generating PureSpectrum Additions\")\n    pure_additions = pureAdds(purespectrum, data)\n    pure_additions.to_excel(writer, sheet_name=\"PureSpectrum - Add\", index=False)\n    format_sheet(writer, \"PureSpectrum - Add\", pure_additions)\n    \n    print(\"Generating PureSpectrum Removes\")\n    pure_removes = pureRemoves(purespectrum, data)  # Corrected typo in 'purespectrrum'\n    pure_removes.to_excel(writer, sheet_name=\"PureSpectrum - Remove\", index=False)\n    format_sheet(writer, \"PureSpectrum - Remove\", pure_removes)\n    \n    # Distribution reconciliations\n    print(\"Generating Distribution Additions\")\n    distribution_additions = distributionAdds(distribution, data)\n    distribution_additions.to_excel(writer, sheet_name=\"Distribution - Add\", index=False)\n    format_sheet(writer, \"Distribution - Add\", distribution_additions)\n    \n    print(\"Generating Distribution Removes\")\n    distribution_removes = distributionRemoves(distribution, data)\n    distribution_removes.to_excel(writer, sheet_name=\"Distribution - Remove\", index=False)\n    format_sheet(writer, \"Distribution - Remove\", distribution_removes)\n    \n    print(f\"File exported as {filename}\")",
    "outputsMetadata": {
     "0": {
      "height": 137,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting the data to an Excel file\n",
      "Generating PureSpectrum Additions\n",
      "Generating PureSpectrum Removes\n",
      "Generating Distribution Additions\n",
      "Generating Distribution Removes\n",
      "File exported as Reconciliation - 02_24_2024.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Get the current date in the format MM_DD_YYYY\n",
    "current_date = datetime.now().strftime(\"%m_%d_%Y\")\n",
    "\n",
    "# Use the current date in the filename\n",
    "filename = f\"Reconciliation - {current_date}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:  # Specify the engine\n",
    "    print(\"Exporting the data to an Excel file\")\n",
    "    \n",
    "    # PureSpectrum reconciliations\n",
    "    if len(purespectrum) > 0:\n",
    "        print(\"Generating PureSpectrum Additions\")\n",
    "        pure_additions = pureAdds(purespectrum, data)\n",
    "        pure_additions.to_excel(writer, sheet_name=\"PureSpectrum - Add\", index=False)\n",
    "        format_sheet(writer, \"PureSpectrum - Add\", pure_additions)\n",
    "        \n",
    "        print(\"Generating PureSpectrum Removes\")\n",
    "        pure_removes = pureRemoves(purespectrum, data)  # Corrected typo in 'purespectrrum'\n",
    "        pure_removes.to_excel(writer, sheet_name=\"PureSpectrum - Remove\", index=False)\n",
    "        format_sheet(writer, \"PureSpectrum - Remove\", pure_removes)\n",
    "    \n",
    "    # Distribution reconciliations\n",
    "    if len(distribution) > 0:\n",
    "        print(\"Generating Distribution Additions\")\n",
    "        distribution_additions = distributionAdds(distribution, data)\n",
    "        distribution_additions.to_excel(writer, sheet_name=\"Distribution - Add\", index=False)\n",
    "        format_sheet(writer, \"Distribution - Add\", distribution_additions)\n",
    "        \n",
    "        print(\"Generating Distribution Removes\")\n",
    "        distribution_removes = distributionRemoves(distribution, data)\n",
    "        distribution_removes.to_excel(writer, sheet_name=\"Distribution - Remove\", index=False)\n",
    "        format_sheet(writer, \"Distribution - Remove\", distribution_removes)\n",
    "    \n",
    "    print(f\"File exported as {filename}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Reconciliate - Surveys",
   "widgets": {}
  },
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
